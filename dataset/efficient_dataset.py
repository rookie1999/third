import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from dataset.utils_norm import normalize_data
from tqdm import tqdm


class EfficientEpisodicDataset(Dataset):
    def __init__(self, dataset_path_list, stats, camera_names=['cam_high'], chunk_size=100):
        """
        Args:
            dataset_path_list: æ•°æ®é›†è·¯å¾„åˆ—è¡¨
            stats: å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®
            camera_names: æ‘„åƒå¤´åç§°åˆ—è¡¨
            chunk_size: åŠ¨ä½œé¢„æµ‹é•¿åº¦
            use_cache: (æœ¬ç‰ˆä¿®æ”¹) True=å…¨é‡åŠ è½½åˆ°å†…å­˜(æå¿«); False=ä½¿ç”¨æ—§ç‰ˆç£ç›˜è¯»å–(æ…¢ä½†çœå†…å­˜)
        """
        super().__init__()
        self.stats = stats
        self.camera_names = camera_names
        self.chunk_size = chunk_size
        self.dataset_path_list = dataset_path_list

        self.episodes = []
        self.indices = []

        print(f"ğŸš€ Pre-loading {len(dataset_path_list)} episodes into RAM (UInt8 mode)...")
        # --- æ¨¡å¼ A: å†…å­˜å…¨é‡åŠ è½½ (æé€Ÿæ¨¡å¼) ---
        for ep_idx, path in enumerate(tqdm(dataset_path_list)):
            with h5py.File(path, 'r') as f:
                # 1. è¯»å–åŸºç¡€æ•°æ®
                qpos = f['observations/qpos'][:]
                action = f['action'][:]

                # 2. è¯»å–å›¾åƒ (ä¿æŒ uint8)
                image_dict = {}
                for cam in camera_names:
                    img_data = f[f'observations/images/{cam}'][:]
                    # ç»Ÿä¸€è½¬æ¢ä¸º (T, C, H, W) æ ¼å¼
                    if img_data.shape[-1] == 3:  # å¦‚æœæ˜¯ (T, H, W, C)
                        img_data = img_data.transpose(0, 3, 1, 2)
                    image_dict[cam] = img_data

                episode_len = len(qpos)
                self.episodes.append({
                    'qpos': qpos,
                    'action': action,
                    'images': image_dict,
                    'len': episode_len
                })

                # å»ºç«‹ç´¢å¼•
                for t in range(episode_len):
                    self.indices.append((ep_idx, t))
        print(f"âœ… Loaded {len(self.indices)} samples. RAM optimized.")

    def __len__(self):
        return len(self.indices)

    def _get_file_handle(self, path):
        if path not in self._file_handles:
            self._file_handles[path] = h5py.File(path, 'r', swmr=True, libver='latest')
        return self._file_handles[path]

    def __getitem__(self, index):
        ep_idx, start_ts = self.indices[index]
        episode = self.episodes[ep_idx]

        # 1. Qpos
        qpos = episode['qpos'][start_ts]
        qpos = normalize_data(qpos, self.stats, 'qpos')
        qpos_tensor = torch.from_numpy(qpos).float()

        # 2. Images (UInt8 -> Float / 255.0)
        imgs = []
        for cam in self.camera_names:
            img_uint8 = episode['images'][cam][start_ts]  # (C, H, W)
            # å®æ—¶å½’ä¸€åŒ– 0-1
            img_float = torch.from_numpy(img_uint8).float() / 255.0
            imgs.append(img_float)
        image_tensor = torch.stack(imgs, dim=0)

        # 3. Action Chunk
        action_full = episode['action']
        action_len = episode['len']

        end_ts = start_ts + self.chunk_size

        if end_ts > action_len:
            # éœ€è¦ Padding
            curr_action = action_full[start_ts:]
            pad_len = end_ts - action_len
            # æ³¨æ„: å¦‚æœæ˜¯ h5py å¯¹è±¡ï¼Œcurr_actionå·²ç»æ˜¯numpy arrayäº†
            last_action = curr_action[-1]
            pad_action = np.repeat(last_action[np.newaxis, :], pad_len, axis=0)
            action_chunk = np.concatenate([curr_action, pad_action], axis=0)
            is_pad = np.zeros(self.chunk_size, dtype=bool)
            is_pad[-pad_len:] = True
        else:
            action_chunk = action_full[start_ts:end_ts]
            is_pad = np.zeros(self.chunk_size, dtype=bool)

        # Action Normalize
        action_chunk = normalize_data(action_chunk, self.stats, 'action')

        return image_tensor, qpos_tensor, torch.from_numpy(action_chunk).float(), torch.from_numpy(is_pad).bool()