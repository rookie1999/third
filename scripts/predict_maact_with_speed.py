import argparse
import pickle
import sys
import time
import collections
import numpy as np
import cv2
import torch
import yaml
import os

# ==========================================
# å¯¼å…¥ SDK Wrapper
# ==========================================
from xarm.wrapper import XArmAPI

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

sys.path.append('/home/lumos/act_move/replay_remote_ctrl')

from scripts.predict import setup_robot
from utils import make_xarm_sdk, make_xarm_reader
from utils.camera import RealSenseCamera
from utils.robot_agent import UniversalRobotAgent

# ==========================================
# å¯¼å…¥ MA-ACT æ¨¡å‹ç»„ä»¶
# ==========================================
# è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ import
from policy.maact.common.model.speed_act_modulate_full_model import SpeedACT
from policy.maact.common.configs.configuration_act import SpeedACTConfig

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
# æœºå™¨äººé…ç½®
CURRENT_ROBOT = 'startouch'
CONFIG_FILE = 'config.yaml'

CKPT_PATH = '/home/lumos/act_move/checkpoints/maact/policy_epoch_699.ckpt'
STATS_PATH = '/home/lumos/act_move/checkpoints/maact/dataset_stats.pkl'

# æ¨ç†å‚æ•°
CHUNK_SIZE = 50  # åŠ¨ä½œå—å¤§å° (ä¸è®­ç»ƒä¿æŒä¸€è‡´)
EXECUTION_HORIZON = 20  # å¼€ç¯æ‰§è¡Œæ­¥æ•° (å°äº Chunk Size)
FREQUENCY = 30  # æ§åˆ¶é¢‘ç‡ Hz
DT = 1.0 / FREQUENCY

N_OBS_STEPS = 1
MAIN_CAMERA_NAME = 'cam_high'  # å¿…é¡»ä¸è®­ç»ƒæ—¶çš„åç§°ä¸€è‡´
CAMERA_NAMES = ['cam_high']
NUM_SPEED_CATEGORIES = 3

def load_checkpoint_compatible(model, checkpoint_path, device):
    """
    è‡ªåŠ¨å¤„ç† DDP è®­ç»ƒå‡ºæ¥çš„ 'module.' å‰ç¼€ï¼Œä½¿å…¶èƒ½åŠ è½½åˆ°å•å¡æ¨¡å‹ä¸­
    """
    print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
    
    # 1. åŠ è½½æ–‡ä»¶
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # 2. å…¼å®¹æ€§å¤„ç†ï¼šæœ‰æ—¶å€™ checkpoint æ˜¯å­—å…¸ï¼Œæƒé‡åœ¨ 'state_dict' é”®é‡Œ
    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint

    # 3. å…³é”®æ­¥éª¤ï¼šå»é™¤ 'module.' å‰ç¼€
    new_state_dict = {}
    for k, v in state_dict.items():
        # å¦‚æœ key ä»¥ 'module.' å¼€å¤´ï¼Œå»æ‰å‰ 7 ä¸ªå­—ç¬¦
        if k.startswith('module.'):
            name = k[7:] 
        else:
            name = k
        new_state_dict[name] = v
        
    # 4. åŠ è½½å¤„ç†åçš„æƒé‡
    msg = model.load_state_dict(new_state_dict, strict=False) # å»ºè®®å…ˆå¼€ False æµ‹è¯•ï¼Œæ²¡é—®é¢˜å† True
    print(f"âœ… Loaded successfully! Missing keys: {msg.missing_keys}")
    return model


def get_user_speed_input(num_categories):
    """
    é˜»å¡å¼è¯¢é—®ç”¨æˆ·å½“å‰çš„é€Ÿåº¦è®¾ç½®ï¼Œæ ¹æ® num_categories åŠ¨æ€ç”ŸæˆéªŒè¯èŒƒå›´
    """
    # åŠ¨æ€ç”Ÿæˆåˆæ³•è¾“å…¥åˆ—è¡¨ï¼Œä¾‹å¦‚ [0, 1, 2, 3, 4]
    valid_range = list(range(num_categories))

    if 1 in valid_range:
        default_val = 1
    elif len(valid_range) > 0:
        default_val = valid_range[len(valid_range) // 2]
    else:
        default_val = 0

    while True:
        print("\n" + "=" * 40)
        print("ğŸš¦ ç­‰å¾…é€Ÿåº¦è®¾ç½® (Wait for Speed Input) ğŸš¦")
        print(f"è¯·è¾“å…¥å½“å‰ä¼ é€å¸¦é€Ÿåº¦ç­‰çº§ (èŒƒå›´: 0 ~ {num_categories - 1}):")
        print("=" * 40)
        try:
            # åŠ¨æ€ç”Ÿæˆæç¤ºå­—ç¬¦ä¸²
            options_str = "/".join(map(str, valid_range))
            prompt = f"ğŸ‘‰ è¯·è¾“å…¥ ({options_str}) [æŒ‰å›è½¦é»˜è®¤ {default_val}]: "

            user_input = input(prompt).strip()

            if user_input == "":
                print(f"æœªè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: [{default_val}]")
                return default_val

            val = int(user_input)
            if val in valid_range:
                print(f"âœ… å·²ç¡®è®¤é€Ÿåº¦ç­‰çº§: [{val}]")
                return val
            else:
                print(f"âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ {valid_range} ä¸­çš„ä¸€ä¸ªæ•°å­—")
        except ValueError:
            print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—")

def main():
    parser = argparse.ArgumentParser(description="ACT Training Script")
    parser.add_argument('--joint_i', action='store_true', help='joint input')
    parser.add_argument('--joint_o', action='store_true', help='joint output')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å½¢çŠ¶: (1, 1, 3, 1, 1) ç”¨äºå¹¿æ’­åŒ¹é… (Batch, Time, Channel, Height, Width)
    NORM_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 1, 3, 1, 1)
    NORM_STD = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 1, 3, 1, 1)

    print(f"Loading stats from {STATS_PATH}...")
    with open(STATS_PATH, 'rb') as f:
        stats = pickle.load(f)
    STATE_DIM = stats["qpos_mean"].shape[0]
    ACTION_DIM = stats["action_mean"].shape[0]
    def pre_process(qpos):
        qpos = qpos[:STATE_DIM]
        return (qpos - stats['qpos_mean']) / stats['qpos_std']
    def post_process(action):
        return action * stats['action_std'] + stats['action_mean']

    print(f"Loading MA-ACT (SpeedACT) model...")

    config = SpeedACTConfig(
        dim_model=512,
        n_heads=8,
        dim_feedforward=3200,
        n_encoder_layers=4,
        n_decoder_layers=1,
        chunk_size=CHUNK_SIZE,
        n_obs_steps=N_OBS_STEPS,
        image_features={cam: (3, 480, 640) for cam in CAMERA_NAMES},
        main_camera=MAIN_CAMERA_NAME,
        robot_state_feature=(STATE_DIM,),
        action_feature=(ACTION_DIM,),
        use_optical_flow=True,
        num_speed_categories=NUM_SPEED_CATEGORIES,
        feedforward_activation="relu",
        pre_norm=False,
        global_flow_size=128,
        optical_flow_map_height=256,
        optical_flow_map_width=320,
    )

    if not os.path.exists(CKPT_PATH):
        print(f"Error: Checkpoint not found at {CKPT_PATH}")
        return

    policy = SpeedACT(config).to(device)
    load_checkpoint_compatible(policy, CKPT_PATH, device)
    policy.eval()
    print("Model loaded successfully.")

    print("Initializing robot and camera...")
    robot = setup_robot(CURRENT_ROBOT, CONFIG_FILE, args.joint_i, args.joint_o, STATE_DIM, ACTION_DIM)
    camera = RealSenseCamera(width=640, height=480, fps=30)

    print("Hardware ready. Starting inference loop...")
    print("Press 'q' in the OpenCV window to quit.")

    try:
        while True:
            current_speed = get_user_speed_input(NUM_SPEED_CATEGORIES)

            speed_tensor = torch.tensor([current_speed], dtype=torch.long, device=device)

            print("ğŸ¤– Robot going home...")
            robot.go_home(blocking=True, duration=3.0)

            print(f"ğŸŸ¢ Start Inference Loop (Speed: {current_speed})... Press [Enter] to Reset.")
            reset_triggered = False
            while not reset_triggered:
                img = camera.get_frame()
                qpos = robot.get_qpos()

                if img is None or qpos is None:
                    time.sleep(0.01)
                    continue

                # --- å›¾åƒå¤„ç†: (H, W, C) -> (1, 1, C, H, W) ---
                # permute: (H, W, C) -> (C, H, W)
                img_tensor = torch.from_numpy(img).float().to(device)
                img_tensor = img_tensor.permute(2, 0, 1)
                # å¢åŠ  Batch å’Œ Time ç»´åº¦
                img_tensor = img_tensor.unsqueeze(0).unsqueeze(0)
                img_tensor = img_tensor / 255.0
                img_tensor = (img_tensor - NORM_MEAN) / NORM_STD

                # --- çŠ¶æ€å¤„ç†: (D,) -> (1, 1, D) ---
                qpos_norm = pre_process(qpos)
                qpos_tensor = torch.from_numpy(qpos_norm).float().to(device)
                qpos_tensor = qpos_tensor.unsqueeze(0).unsqueeze(0)

                # 2. æ¨¡å‹æ¨ç†
                with torch.inference_mode():
                    batch = {
                        "observation.state": qpos_tensor,
                        "observation.images": [img_tensor],
                        "action_is_pad": torch.zeros(1, CHUNK_SIZE, dtype=torch.bool, device=device),
                        "speed_label": speed_tensor
                    }
                    all_actions, _ = policy(batch)

                all_actions = all_actions.squeeze(0).cpu().numpy()
                all_actions = post_process(all_actions)

                # 3. åŠ¨ä½œæ‰§è¡Œå¾ªç¯ (Open-Loop Execution)
                for t in range(EXECUTION_HORIZON):
                    t_exec_start = time.time()

                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q'):
                        print("Quitting...")
                        raise KeyboardInterrupt
                    elif key == 13:  # Enter é”® (ASCII 13)
                        print("\nğŸ”„ Reset triggered! Restarting session...")
                        reset_triggered = True
                        break

                    target_action = all_actions[t]
                    robot.command_action(target_action)

                    # --- æ›´æ–°è§‚æµ‹ (ä»…ç”¨äºæ˜¾ç¤º) ---
                    # å› ä¸ºä¸‹ä¸€è½®æ¨ç†ä¸éœ€è¦è¿™é‡Œçš„å†å²æ•°æ®ï¼Œæ‰€ä»¥åªåšæ˜¾ç¤º
                    curr_img = camera.get_frame()
                    if curr_img is not None:
                        bgr_img = cv2.cvtColor(curr_img, cv2.COLOR_RGB2BGR)
                        # åœ¨å·¦ä¸Šè§’æ˜¾ç¤ºå½“å‰é€Ÿåº¦æ¨¡å¼
                        cv2.putText(bgr_img, f"Speed Mode: {current_speed}", (10, 30),
                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                        cv2.imshow("Camera View", bgr_img)

                    # --- é¢‘ç‡æ§åˆ¶ ---
                    elapsed = time.time() - t_exec_start
                    if elapsed < DT:
                        time.sleep(DT - elapsed)

                # å¦‚æœè§¦å‘äº† Resetï¼Œbreak è·³å‡ºå†…å±‚å¾ªç¯ï¼Œå›åˆ°å¤–å±‚ (Input -> Go Home)
                if reset_triggered:
                    break

    except KeyboardInterrupt:
        print("Stopping...")
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        camera.stop()
        cv2.destroyAllWindows()
        print("Safety exit.")


if __name__ == '__main__':
    main()