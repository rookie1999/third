import argparse
import pickle
import sys
import time
import collections
import numpy as np
import cv2
import torch
import yaml
import os

# ==========================================
# å¯¼å…¥ SDK Wrapper
# ==========================================
from xarm.wrapper import XArmAPI

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

sys.path.append('/home/lumos/act_move/replay_remote_ctrl')

from scripts.predict import setup_robot
from utils import make_xarm_sdk, make_xarm_reader
from utils.camera import RealSenseCamera
from utils.robot_agent import UniversalRobotAgent

# ==========================================
# å¯¼å…¥ MA-ACT æ¨¡å‹ç»„ä»¶
# ==========================================
# è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ import
from policy.maact.common.model.speed_act_modulate_full_model import SpeedACT
from policy.maact.common.configs.configuration_act import SpeedACTConfig

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
# æœºå™¨äººé…ç½®
CURRENT_ROBOT = 'startouch'
CONFIG_FILE = 'config.yaml'

# æ¨¡å‹è·¯å¾„ (è¯·ç¡®ä¿æŒ‡å‘ MA-ACT è®­ç»ƒå¥½çš„æƒé‡)
CKPT_PATH = '/home/lumos/act_move/checkpoints/maact/policy_epoch_699.ckpt'
STATS_PATH = '/home/lumos/act_move/checkpoints/maact/dataset_stats.pkl'
# YOLO æƒé‡è·¯å¾„ (å¿…é¡»å­˜åœ¨)
# YOLO_CKPT = r"F:\projects\lumos\ma_act\src\object_detection\object_detection_ckpt\yolov8n.pt"

# æ¨ç†å‚æ•°
CHUNK_SIZE = 50  # åŠ¨ä½œå—å¤§å° (ä¸è®­ç»ƒä¿æŒä¸€è‡´)
EXECUTION_HORIZON = 20  # å¼€ç¯æ‰§è¡Œæ­¥æ•° (å°äº Chunk Size)
FREQUENCY = 30  # æ§åˆ¶é¢‘ç‡ Hz
DT = 1.0 / FREQUENCY

# MA-ACT å¿…é¡»è‡³å°‘2å¸§å†å²
N_OBS_STEPS = 2
MAIN_CAMERA_NAME = 'cam_high'  # å¿…é¡»ä¸è®­ç»ƒæ—¶çš„åç§°ä¸€è‡´
CAMERA_NAMES = ['cam_high']



def load_checkpoint_compatible(model, checkpoint_path, device):
    """
    è‡ªåŠ¨å¤„ç† DDP è®­ç»ƒå‡ºæ¥çš„ 'module.' å‰ç¼€ï¼Œä½¿å…¶èƒ½åŠ è½½åˆ°å•å¡æ¨¡å‹ä¸­
    """
    print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
    
    # 1. åŠ è½½æ–‡ä»¶
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # 2. å…¼å®¹æ€§å¤„ç†ï¼šæœ‰æ—¶å€™ checkpoint æ˜¯å­—å…¸ï¼Œæƒé‡åœ¨ 'state_dict' é”®é‡Œ
    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint

    # 3. å…³é”®æ­¥éª¤ï¼šå»é™¤ 'module.' å‰ç¼€
    new_state_dict = {}
    for k, v in state_dict.items():
        # å¦‚æœ key ä»¥ 'module.' å¼€å¤´ï¼Œå»æ‰å‰ 7 ä¸ªå­—ç¬¦
        if k.startswith('module.'):
            name = k[7:] 
        else:
            name = k
        new_state_dict[name] = v
        
    # 4. åŠ è½½å¤„ç†åçš„æƒé‡
    msg = model.load_state_dict(new_state_dict, strict=False) # å»ºè®®å…ˆå¼€ False æµ‹è¯•ï¼Œæ²¡é—®é¢˜å† True
    print(f"âœ… Loaded successfully! Missing keys: {msg.missing_keys}")
    return model




def main():
    parser = argparse.ArgumentParser(description="ACT Training Script")
    parser.add_argument('--joint_i', action='store_true', help='joint input')
    parser.add_argument('--joint_o', action='store_true', help='joint output')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å½¢çŠ¶: (1, 1, 3, 1, 1) ç”¨äºå¹¿æ’­åŒ¹é… (Batch, Time, Channel, Height, Width)
    NORM_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 1, 3, 1, 1)
    NORM_STD = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 1, 3, 1, 1)

    # -------------------------------------------------------------------------
    # 2. åŠ è½½ç»Ÿè®¡æ•°æ®
    # -------------------------------------------------------------------------
    print(f"Loading stats from {STATS_PATH}...")
    with open(STATS_PATH, 'rb') as f:
        stats = pickle.load(f)
    STATE_DIM = stats["qpos_mean"].shape[0]
    ACTION_DIM = stats["action_mean"].shape[0]
    # å®šä¹‰é¢„å¤„ç†å’Œåå¤„ç†
    def pre_process(qpos):
        qpos = qpos[:STATE_DIM]
        return (qpos - stats['qpos_mean']) / stats['qpos_std']

    def post_process(action):
        return action * stats['action_std'] + stats['action_mean']

    # -------------------------------------------------------------------------
    # 3. åˆå§‹åŒ– SpeedACT æ¨¡å‹
    # -------------------------------------------------------------------------
    print(f"Loading MA-ACT (SpeedACT) model...")

    # config = SpeedACTConfig(
    #     dim_model=512,
    #     chunk_size=CHUNK_SIZE,
    #     n_obs_steps=N_OBS_STEPS,
    #     # æ³¨æ„ï¼šå›¾åƒå°ºå¯¸ (480, 640) å¿…é¡»ä¸ RealSenseCamera è®¾ç½®ä¸€è‡´
    #     image_features={MAIN_CAMERA_NAME: (3, 480, 640)},
    #     main_camera=MAIN_CAMERA_NAME,

    #     # [å…³é”®ä¿®æ­£] ç»´åº¦éœ€åŒ¹é… train_maact.py
    #     robot_state_feature=(STATE_DIM,),
    #     action_feature=(ACTION_DIM,),

    #     use_optical_flow=True,
    #     # object_detection_ckpt_path=YOLO_CKPT,
    #     # cropped_flow_h=64,
    #     # cropped_flow_w=64,
    #     feedforward_activation="relu",
    #     pre_norm=False
    # )
    config = SpeedACTConfig(
        dim_model=512,
        n_heads=8,
        dim_feedforward=3200,
        n_encoder_layers=4,
        n_decoder_layers=1,
        chunk_size=CHUNK_SIZE,
        n_obs_steps=N_OBS_STEPS,
        image_features={cam: (3, 480, 640) for cam in CAMERA_NAMES},
        main_camera=MAIN_CAMERA_NAME,
        robot_state_feature=(STATE_DIM,),
        action_feature=(ACTION_DIM,),
        use_optical_flow=True,
        feedforward_activation="relu",
        pre_norm=False,
        global_flow_size=128,
        optical_flow_map_height=256,
        optical_flow_map_width=320,
    )

    policy = SpeedACT(config)

    # åŠ è½½æƒé‡
    if not os.path.exists(CKPT_PATH):
        print(f"Error: Checkpoint not found at {CKPT_PATH}")
        return

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy = SpeedACT(config).to(device)
    load_checkpoint_compatible(policy, CKPT_PATH, device)
    policy.eval()
    print("Model loaded successfully.")

    # -------------------------------------------------------------------------
    # 4. åˆå§‹åŒ–ç¡¬ä»¶
    # -------------------------------------------------------------------------
    print("Initializing robot and camera...")
    robot = setup_robot(CURRENT_ROBOT, CONFIG_FILE, args.joint_i, args.joint_o)
    # ç¡®ä¿åˆ†è¾¨ç‡ä¸ Config ä¸€è‡´
    camera = RealSenseCamera(width=640, height=480, fps=30)
    # camera.start()

    # é¢„çƒ­ç›¸æœº
    for _ in range(10):
        camera.get_frame()
        time.sleep(0.1)

    print("Hardware ready. Starting inference loop...")
    print("Press 'q' in the OpenCV window to quit.")

    # -------------------------------------------------------------------------
    # 5. æ¨ç†ä¸»å¾ªç¯
    # -------------------------------------------------------------------------
    # å†å²è§‚æµ‹ç¼“å†²åŒº: è‡ªåŠ¨ä¿æŒæœ€è¿‘ N_OBS_STEPS å¸§
    obs_history = collections.deque(maxlen=N_OBS_STEPS)

    print("Warming up observation buffer...")
    for _ in range(N_OBS_STEPS):
        t0 = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        img = camera.get_frame()
        qpos = robot.get_qpos()
        if img is not None and qpos is not None:
            obs_history.append({'image': img, 'qpos': pre_process(qpos)})

        # æ‰£é™¤æ‰§è¡Œæ—¶é—´ï¼Œç²¾ç¡®ç­‰å¾…
        elapsed = time.time() - t0
        if elapsed < DT:
            time.sleep(DT - elapsed)

    try:
        while True:
            # 1. å †å å›¾åƒ: (T, H, W, C) -> (T, C, H, W)
            img_seq = np.stack([x['image'] for x in obs_history])
            img_seq = np.transpose(img_seq, (0, 3, 1, 2))
            img_tensor = torch.from_numpy(img_seq).float().to(device) / 255.0
            img_tensor = img_tensor.unsqueeze(0)  # (1, T, C, H, W)

            # ImageNet å½’ä¸€åŒ–
            img_tensor = (img_tensor - NORM_MEAN) / NORM_STD

            # 2. å †å çŠ¶æ€
            qpos_seq = np.stack([x['qpos'] for x in obs_history])
            qpos_tensor = torch.from_numpy(qpos_seq).float().to(device).unsqueeze(0)

            # 3. æ¨¡å‹å‰å‘æ¨ç†
            with torch.inference_mode():
                batch = {
                    "observation.state": qpos_tensor,
                    "observation.images": [img_tensor],
                    MAIN_CAMERA_NAME: img_tensor,
                    "action_is_pad": torch.zeros(1, CHUNK_SIZE, dtype=torch.bool, device=device)
                }
                # SpeedACT è¿”å›4ä¸ªå€¼ï¼Œåªå–ç¬¬ä¸€ä¸ª
                s = time.time()
                all_actions = policy(batch)[0]
                e = time.time()
                print(e - s)

            # åå½’ä¸€åŒ–
            all_actions = all_actions.squeeze(0).cpu().numpy()
            all_actions = post_process(all_actions)

            for t in range(EXECUTION_HORIZON):
                loop_start = time.time()

                # A. å‘é€æŒ‡ä»¤
                target_action = all_actions[t]
                robot.command_action(target_action)

                curr_img = camera.get_frame()
                curr_qpos = robot.get_qpos()

                if curr_img is not None and curr_qpos is not None:
                    # cv2.imshow("Camera View", curr_img)
                    # cv2.imshow("Camera View", cv2.cvtColor(curr_img, cv2.COLOR_RGB2BGR))
                    obs_history.append({'image': curr_img, 'qpos': pre_process(curr_qpos)})

                # C. é¢‘ç‡æ§åˆ¶
                loop_elapsed = time.time() - loop_start
                if loop_elapsed < DT:
                    time.sleep(DT - loop_elapsed)

                # D. å“åº”é€€å‡º
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    raise KeyboardInterrupt

    except KeyboardInterrupt:
        print("Stopping...")
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        camera.stop()
        cv2.destroyAllWindows()
        print("Safety exit.")


if __name__ == '__main__':
    main()